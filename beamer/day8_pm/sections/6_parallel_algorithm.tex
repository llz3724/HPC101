\section{Parallel Algorithms}

\subsection{Parallelizing the Standard Library Algorithms}
\begin{frame}[fragile]{C++17 Parallel Algorithms}
	\textbf{Enhancement to STL Algorithms}: Parallel execution support

	\begin{minted}{cpp}
#include <execution>

std::vector<int> my_data = {3, 1, 4, 1, 5, 9, 2, 6};

// Serial execution (C++11/14)
std::sort(my_data.begin(), my_data.end());

// Parallel execution (C++17)
std::sort(std::execution::par, my_data.begin(), my_data.end());
	\end{minted}

	\textbf{Key Change}: Additional first parameter for execution policy

	\vspace{0.5em}
	\textbf{Available for}: \texttt{std::sort}, \texttt{std::for\_each}, \texttt{std::transform}, \texttt{std::reduce}, and many more!
\end{frame}

\begin{frame}[fragile]{Similarity to OpenMP}
	\begin{minted}[fontsize=\scriptsize]{cpp}
#pragma omp parallel for
for(unsigned i=0;i<my_data.size();++i)
{
    do_stuff(my_data[i]);
}

std::for_each(std::execution::par, my_data.begin(), my_data.end(), do_stuff);
    \end{minted}
\end{frame}

\subsection{Execution Policies}
\begin{frame}[fragile]{Execution Policy Types}
	\textbf{Four execution policies available}:

	\begin{minted}{cpp}
// 1. Sequential (same as no policy)
std::execution::sequenced_policy      // std::execution::seq
// 2. Parallel
std::execution::parallel_policy       // std::execution::par
// 3. Parallel + Vectorized
std::execution::parallel_unsequenced_policy  // std::execution::par_unseq
// 4. Unsequenced (C++20)
std::execution::unsequenced_policy    // std::execution::unseq
std::sort(std::execution::seq, v.begin(), v.end());
std::sort(std::execution::par, v.begin(), v.end());
std::sort(std::execution::par_unseq, v.begin(), v.end());
	\end{minted}

	Execution policies is a \textbf{permission}, not a \textbf{requirement}.
\end{frame}

\begin{frame}{General Effects of Execution Policies}
	\textbf{std::execution::sequenced\_policy}:
	\begin{itemize}
		\item Same as traditional serial execution
		\item Single thread, sequential order
	\end{itemize}

	\textbf{std::execution::parallel\_policy}:
	\begin{itemize}
		\item Multiple threads allowed
		\item Operations may be interleaved
		\item No vectorization guarantees
	\end{itemize}
\end{frame}

\begin{frame}{General Effects of Execution Policies}

	\textbf{std::execution::parallel\_unsequenced\_policy}:
	\begin{itemize}
		\item Multiple threads + vectorization
		\item Operations may execute in any order
		\item Best performance potential
		\item Requires thread-safe operations
	\end{itemize}
\end{frame}

\setminted{
	fontsize=\scriptsize
}
\subsection{The Parallel Algorithms from the C++ Standard Library}
\begin{frame}[fragile]{Examples of Using Parallel Algorithms}
	\textbf{Parallel Transform}:
	\begin{minted}{cpp}
std::vector<int> input = {1, 2, 3, 4, 5};
std::vector<int> output(input.size());

std::transform(std::execution::par,
               input.begin(), input.end(),
               output.begin(),
               [](int x) { return x * x; });
	\end{minted}
\end{frame}

\begin{frame}[fragile]{Examples of Using Parallel Algorithms}
	\textbf{Parallel For Each}:
	\begin{minted}{cpp}
std::vector<std::string> words = {"hello", "world", "parallel"};

std::for_each(std::execution::par,
              words.begin(), words.end(),
              [](std::string& word) {
                  std::transform(word.begin(), word.end(),
                                word.begin(), ::toupper);
              });
	\end{minted}
\end{frame}

\begin{frame}[fragile]{Examples of Using Parallel Algorithms}
	\textbf{Parallel Reduction}:
	\begin{minted}{cpp}
std::vector<int> numbers(1000000);
std::iota(numbers.begin(), numbers.end(), 1);

// Parallel sum
auto sum = std::reduce(std::execution::par,
                       numbers.begin(), numbers.end(),
                       0);

// Parallel sum with custom operation
auto product = std::reduce(std::execution::par,
                          numbers.begin(), numbers.end(),
                          1,
                          std::multiplies<int>());
	\end{minted}
\end{frame}

\begin{frame}[fragile]{Examples of Using Parallel Algorithms}
	\textbf{Transform Reduce}:
	\begin{minted}{cpp}
auto squared_sum = std::transform_reduce(
    std::execution::par,
    numbers.begin(), numbers.end(),
    0,
    std::plus<>(),
    [](int x) { return x * x; }
);
	\end{minted}
\end{frame}

\setminted{
	fontsize=\tiny
}

\begin{frame}[fragile]{Counting Visits Example}
	\textbf{Website Analytics - Parallel Processing}:

	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{minted}{cpp}
struct Visit {
    std::string page;
    int user_id;
    std::chrono::time_point<std::chrono::system_clock> timestamp;
};

std::vector<Visit> visits = load_visits();

// Count unique pages visited in parallel
std::sort(std::execution::par, visits.begin(), visits.end(),
          [](const Visit& a, const Visit& b) {
              return a.page < b.page;
          });
    \end{minted}
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{minted}{cpp}
// Count visits per page
std::map<std::string, int> page_counts;
auto current = visits.begin();
while (current != visits.end()) {
    auto range_end = std::upper_bound(
        std::execution::par,
        current, visits.end(), *current,
        [](const Visit& a, const Visit& b) {
            return a.page < b.page;
        });

    page_counts[current->page] =
        std::distance(current, range_end);
    current = range_end;
}
            \end{minted}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[fragile]{Performance Considerations}
	\textbf{When to Use Parallel Algorithms}:
	\begin{itemize}
		\item Large datasets (typically > 10,000 elements)
		\item CPU-intensive operations
		\item Independent computations
	\end{itemize}

	\textbf{Overhead Example}:
	\begin{minted}{cpp}
// Small dataset - sequential might be faster
std::vector<int> small_data(100);
std::sort(std::execution::seq, small_data.begin(), small_data.end());

// Large dataset - parallel beneficial
std::vector<int> large_data(1000000);
std::sort(std::execution::par, large_data.begin(), large_data.end());
	\end{minted}
\end{frame}

\begin{frame}{Thread Safety in Parallel Algorithms}
	\textbf{Thread Safety Requirements}:
	\begin{itemize}
		\item Operations must be thread-safe for \texttt{par} and \texttt{par\_unseq}
		\item Avoid shared mutable state
		\item Use atomic operations when necessary
	\end{itemize}
\end{frame}

\subsection{Assignment: GPU Execution with NVIDIA}
\begin{frame}{Homework Assignment}
	\textbf{Research Task}:
	\begin{enumerate}
		\item Review NVIDIA compiler manual (nvc++)
		\item Study how to run C++ standard algorithms on GPU
		\item Observe CUDA calls generated by the compiler
	\end{enumerate}

	\textbf{Experiment Steps}:
	\begin{enumerate}
		\item Load NVIDIA HPC SDK
		\item Compile parallel algorithms with GPU support
		\item Use correct compile option
		\item Analyze generated CUDA code
	\end{enumerate}
\end{frame}

\begin{frame}{Homework Assignment}
	\textbf{Expected Results}:
	\begin{itemize}
		\item Standard C++ code runs without modification
		\item GPU kernel functions are auto-generated
		\item \textcolor{gray}{Significant performance improvement observed}
	\end{itemize}

	\vspace{0.5em}
	\textbf{Tip}: Use \texttt{nvprof} or \texttt{nsys} for performance analysis
\end{frame}

\begin{frame}[fragile]{Assignment Example Code}
	\textbf{Compile Command}: \verb|nvc++ <correct_option> -O3 test.cpp -o test|
	\textbf{Test Code Template}:

	\begin{minted}{cpp}
const size_t N = 10'000'000;
std::vector<int> data(N);
std::iota(data.begin(), data.end(), 1);
std::random_shuffle(data.begin(), data.end());

auto start = std::chrono::high_resolution_clock::now();

// This should run on GPU with nvc++ and correct options
std::sort(std::execution::par_unseq,
            data.begin(), data.end());

auto end = std::chrono::high_resolution_clock::now();
auto duration = std::chrono::duration_cast<
    std::chrono::milliseconds>(end - start);

std::cout << "Sorting took: " << duration.count()
            << " ms" << std::endl;
    \end{minted}

\end{frame}
